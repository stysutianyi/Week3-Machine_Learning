{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IAT-ExploringAI-2024/Week3-Machine_Learning/blob/main/Week_3_MachineLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5CvOjjrL9gw"
      },
      "source": [
        "<center><h1> Introduction to Audio Classification with Machine Learning Models </h1></center>\n",
        "\n",
        "### Purpose\n",
        "\n",
        "This notebook serves as an introduction to working with audio data for classification problems; it is meant as a learning resource rather than a demonstration of the state-of-the-art. The techniques mentioned in this notebook apply not only to classification problems, but to regression problems and problems dealing with other types of input data as well. I provide an introduction to a few key machine learning models and the logic in choosing their hyperparameters. These objectives are framed by the task of recognizing emotion from snippets of speech audio.\n",
        "\n",
        "Training data should be used strictly for training a model, validation data strictly for tuning a model, and test data strictly to evaluate a model once it is tuned - a model should never be tuned to perform better on test data.\n",
        "\n",
        "Classic machine learning models such as Support Vector Machines (SVM), k Nearest Neighbours (kNN), and Random Forests have distinct advantages to deep neural networks in many tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQsTfGREL9g1"
      },
      "source": [
        "<!--TABLE OF CONTENTS-->\n",
        "\n",
        "# Table of Contents\n",
        "\n",
        "- [Intro: Speech Emotion Recognition on the RAVDESS dataset](#Intro:-Speech-Emotion-Recognition-on-the-RAVDESS-dataset)\n",
        "- [Machine Learning Process Overview](#Machine-Learning-Process-Overview)\n",
        "- [Feature Extraction](#Feature-Extraction)\n",
        "  - [Load the Dataset and Compute Features](#Load-the-Dataset-and-Compute-Features)\n",
        "  - [Feature Scaling](#Feature-Scaling)\n",
        "- [Classical Machine Learning Models](#Classical-Machine-Learning-Models)\n",
        "  - [Training: The 80/20 Split and Validation](#Training:-The-80/20-Split-and-Validation)\n",
        "  - [Comparing Models](#Comparing-Models)\n",
        "  - [The Support Vector Machine Classifier](#The-Support-Vector-Machine-Classifier)\n",
        "  - [k Nearest Neighbours](#k-Nearest-Neighbours)\n",
        "  - [Random Forests](#Random-Forests)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZUcbr4PL9g2",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Intro: Speech Emotion Recognition on the RAVDESS dataset\n",
        "\n",
        "In this notebook we explore the most common machine learning models, specifically those available off the shelf in scikit-learn.\n",
        "\n",
        "I'm going to use the RAVDESS dataset (Ryerson Audio-Visual Database of Emotional Speech and Song dataset), created by Steven Livingstone and Frank Russo of Ryerson University. <br>\n",
        "[Details of the RAVDESS dataset](https://smartlaboratory.org/ravdess/) <br>\n",
        "[Download the dataset used in this notebook](https://zenodo.org/record/1188976) <br> Scroll half-way down the page and find \"Audio_Speech_Actors_01-24\"<br>\n",
        "\n",
        "We're going to use the audio-only speech portion of the RAVDESS dataset, ~200MB.\n",
        "Audio is sourced from 24 actors (12 male, 12 female) repeating two sentences with\n",
        "a variety of emotions and intensity. We get 1440 speech files (24 actors \\* 60 recordings per actor). Each audio sample has been rated by a human 10 times for emotional quality.\n",
        "\n",
        "## Machine Learning Process Overview\n",
        "\n",
        "1. Feature Engineering: Choose and define the properties which our model will use to evaluate the audio files. <br>\n",
        "2. Feature Extraction: Compute the features for each audio file and build a feature matrix representing all audio files. <br>\n",
        "3. Model exploration: Test candidate models that make sense for the properies of the dataset\n",
        "4. Training the MLP Classifier model: Choose and optimize the properties of our model on validation data - hyperparameters and architechture. <br>\n",
        "5. Evaluate our model's performance: Evaluate our model's accuracy on validation data and score it against test data which it has never seen in training.<br>\n",
        "6. Explore options for improving our model: Is our dataset the right size? Is our model too complex or too simple? <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "b2IL0uT19_3A"
      },
      "outputs": [],
      "source": [
        "# importing the required libraries\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa.display\n",
        "import soundfile\n",
        "import os\n",
        "\n",
        "# matplotlib complains about the behaviour of librosa.display, so we'll ignore those warnings:\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu8VPhDmL9hC",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Feature Extraction\n",
        "\n",
        "We're going to take full advantage of librosa, a Python library enabling audio analysis and feature extraction.\n",
        "Librosa abstracts away all the math and most of the details of mel spectrorgams, chromagrams, and MFCC.\n",
        "Although closely related, we're going to take the Mel Spectrogram, MFCC, and chromagrams of each audio file as separate features to try\n",
        "and have bit more discriminatory power between samples. <br>\n",
        "\n",
        "Let's build our feature extraction functions to get a chromagram, a mel spectorgram, and MFC coefficients for each of our audio files. Because the chromagram, mel spectrogram and MFCCs are calculated on audio frames produced by STFT, we're going to get a matrix back from each function, so we'll take the mean of those matrices to produce a single feature array for each feature and each audio sample, i.e. 3 feature arrays per audio sample.\n",
        "\n",
        "**Chromagram**: Will produce 12 features; One for each of 12 pitch classes\n",
        "\n",
        "**Mel Spectrogram**: Will produce 128 features; We've defined the number of mel frequency bands at n_mels=128\n",
        "\n",
        "**MFCC**: Will produce 40 MFCCs; I've set the number of coefficients to return at n_mfcc=40 which I found to work well\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qTe93WYTL9hD",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def feature_chromagram(waveform, sample_rate):\n",
        "    # STFT computed here explicitly; mel spectrogram and MFCC functions do this under the hood\n",
        "    stft_spectrogram = np.abs(librosa.stft(waveform))\n",
        "    # Produce the chromagram for all STFT frames and get the mean of each column of the resulting matrix to create a feature array\n",
        "    chromagram = np.mean(\n",
        "        librosa.feature.chroma_stft(S=stft_spectrogram, sr=sample_rate).T, axis=0\n",
        "    )\n",
        "    return chromagram\n",
        "\n",
        "\n",
        "def feature_melspectrogram(waveform, sample_rate):\n",
        "    # Produce the mel spectrogram for all STFT frames and get the mean of each column of the resulting matrix to create a feature array\n",
        "    # Using 8khz as upper frequency bound should be enough for most speech classification tasks\n",
        "    melspectrogram = np.mean(\n",
        "        librosa.feature.melspectrogram(\n",
        "            y=waveform, sr=sample_rate, n_mels=128, fmax=8000\n",
        "        ).T,\n",
        "        axis=0,\n",
        "    )\n",
        "    return melspectrogram\n",
        "\n",
        "\n",
        "def feature_mfcc(waveform, sample_rate):\n",
        "    # Compute the MFCCs for all STFT frames and get the mean of each column of the resulting matrix to create a feature array\n",
        "    # 40 filterbanks = 40 coefficients\n",
        "    mfc_coefficients = np.mean(\n",
        "        librosa.feature.mfcc(y=waveform, sr=sample_rate, n_mfcc=40).T, axis=0\n",
        "    )\n",
        "    return mfc_coefficients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjFXIfC2L9hD",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "We're going to wrap our feature extraction functions so we only have to load each audio file once. After extracting our 3 audio features as NumPy arrays representing a time series, we're going to\n",
        "stack them horizontally to create a single feature array.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "xPMw9ijJL9hE",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def get_features(file):\n",
        "    # load an individual soundfile\n",
        "    with soundfile.SoundFile(file) as audio:\n",
        "        waveform = audio.read(dtype=\"float32\")\n",
        "        sample_rate = audio.samplerate\n",
        "        # compute features of soundfile\n",
        "        chromagram = feature_chromagram(waveform, sample_rate)\n",
        "        melspectrogram = feature_melspectrogram(waveform, sample_rate)\n",
        "        mfc_coefficients = feature_mfcc(waveform, sample_rate)\n",
        "\n",
        "        feature_matrix = np.array([])\n",
        "        # use np.hstack to stack our feature arrays horizontally to create a feature matrix\n",
        "        feature_matrix = np.hstack((chromagram, melspectrogram, mfc_coefficients))\n",
        "\n",
        "        return feature_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-C6g6psL9hE",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Load the Dataset and Compute Features\n",
        "\n",
        "We have to understand the labelling of the RAVDESS dataset to find the ground truth emotion for each sample.\n",
        "Each file is labelled with 7 numbers delimited by a \"-\".\n",
        "Most of the numbers describe metadata about the audio samples such as their format (video and/or audio),\n",
        "whether the audio is a song or statement, which of two statements is being read and by which actor.\n",
        "\n",
        "The third and fourth numbers pertain to the emotional quality of each sample. The third number is in the range of 1-8 with each number representing an emotion.\n",
        "The fourth number is either 1 or 2, representing normal (1) or strong (2) emotional intensity.\n",
        "\n",
        "We're going to define a dictionary based on the third number (emotion) and assign an emotion to each number as specified by the RAVDESS dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "z-Pu_fB7L9hF",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Emotions in the RAVDESS dataset\n",
        "emotionsRAVDESS = {\n",
        "    \"01\": \"neutral\",\n",
        "    # \"02\": \"calm\",\n",
        "    \"03\": \"happy\",\n",
        "    \"04\": \"sad\",\n",
        "    \"05\": \"angry\",\n",
        "    \"06\": \"fearful\",\n",
        "    \"07\": \"disgust\",\n",
        "    \"08\": \"surprised\",\n",
        "}\n",
        "\n",
        "emotionsTESS = {\n",
        "    \"angry\": \"angry\",\n",
        "    \"disgust\": \"disgust\",\n",
        "    \"fear\": \"fearful\",\n",
        "    \"happy\": \"happy\",\n",
        "    \"neutral\": \"neutral\",\n",
        "    \"ps\": \"surprised\",\n",
        "    \"sad\": \"sad\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTlsUOwXL9hF",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Finally, let's load our entire dataset and compute the features of each audio file:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "mAh2AYMpL9hF",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import os, glob\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    X, y = [], []\n",
        "    count = 0\n",
        "    # Process RAVDESS dataset\n",
        "    for file in glob.glob(\"MIX/Audio_Speech_Actors_01-24/Actor_*/*.wav\"):\n",
        "        file_name = os.path.basename(file)\n",
        "        emotion_code = file_name.split(\"-\")[2]\n",
        "        if emotion_code in emotionsRAVDESS:\n",
        "            emotion = emotionsRAVDESS[emotion_code]\n",
        "            gender = \"male\" if int(file_name.split(\"-\")[6][1]) % 2 else \"female\"\n",
        "            features = get_features(file)\n",
        "            X.append(features)\n",
        "            y.append(emotion)\n",
        "        else:\n",
        "            print(f\"Skipping file {file_name} due to unwanted emotion code.\")\n",
        "        count += 1\n",
        "        print(f\"\\rProcessed RAVDESS: {count} audio samples\", end=\" \")\n",
        "\n",
        "    # Process TESS dataset\n",
        "    for folder in glob.glob(\"MIX/TESS/*\"):\n",
        "        for file in glob.glob(f\"{folder}/*.wav\"):\n",
        "            folder_name = os.path.basename(folder)\n",
        "            emotion = emotionsTESS[folder_name.split(\"_\")[1].lower()]\n",
        "            gender = \"male\" if \"OAF\" in folder_name else \"female\"\n",
        "            features = get_features(file)\n",
        "            X.append(features)\n",
        "            y.append(emotion)\n",
        "            count += 1\n",
        "            print(f\"\\rProcessed TESS: {count} audio samples\", end=\" \")\n",
        "\n",
        "    return np.array(X), np.array(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Cf8q4K5L9hG",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Compute the feature matrix and read the emotion labels for the entire dataset.\n",
        "Note that our regressor (independent/explanatory variable), usually denoted X, is named 'features', and our regressand (dependent variable), usually denoted y, is named 'emotions'.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeKE591aL9hG",
        "outputId": "ae181c7d-c0bb-4263-84fc-cf79f71e8169",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed RAVDESS: 12 audio samples Skipping file 03-01-02-02-01-01-16.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 14 audio samples Skipping file 03-01-02-01-02-01-16.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 17 audio samples Skipping file 03-01-02-02-02-02-16.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 18 audio samples Skipping file 03-01-02-01-01-02-16.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 30 audio samples Skipping file 03-01-02-01-02-02-16.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 32 audio samples Skipping file 03-01-02-02-01-02-16.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 56 audio samples Skipping file 03-01-02-01-01-01-16.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 57 audio samples Skipping file 03-01-02-02-02-01-16.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 61 audio samples Skipping file 03-01-02-01-02-01-11.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 62 audio samples Skipping file 03-01-02-02-01-01-11.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 85 audio samples Skipping file 03-01-02-01-01-02-11.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 87 audio samples Skipping file 03-01-02-02-02-02-11.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 102 audio samples Skipping file 03-01-02-02-01-02-11.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 103 audio samples Skipping file 03-01-02-01-02-02-11.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 107 audio samples Skipping file 03-01-02-02-02-01-11.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 109 audio samples Skipping file 03-01-02-01-01-01-11.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 122 audio samples Skipping file 03-01-02-02-02-01-18.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 123 audio samples Skipping file 03-01-02-01-01-01-18.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 147 audio samples Skipping file 03-01-02-02-01-02-18.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 149 audio samples Skipping file 03-01-02-01-02-02-18.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 161 audio samples Skipping file 03-01-02-01-01-02-18.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 162 audio samples Skipping file 03-01-02-02-02-02-18.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 165 audio samples Skipping file 03-01-02-01-02-01-18.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 167 audio samples Skipping file 03-01-02-02-01-01-18.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 190 audio samples Skipping file 03-01-02-01-01-01-20.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 191 audio samples Skipping file 03-01-02-02-02-01-20.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 194 audio samples Skipping file 03-01-02-01-02-02-20.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 196 audio samples Skipping file 03-01-02-02-01-02-20.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 210 audio samples Skipping file 03-01-02-02-02-02-20.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 211 audio samples Skipping file 03-01-02-01-01-02-20.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 235 audio samples Skipping file 03-01-02-02-01-01-20.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 237 audio samples Skipping file 03-01-02-01-02-01-20.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 250 audio samples Skipping file 03-01-02-02-02-01-21.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 251 audio samples Skipping file 03-01-02-01-01-01-21.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 255 audio samples Skipping file 03-01-02-02-01-02-21.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 257 audio samples Skipping file 03-01-02-01-02-02-21.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 269 audio samples Skipping file 03-01-02-01-01-02-21.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 270 audio samples Skipping file 03-01-02-02-02-02-21.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 293 audio samples Skipping file 03-01-02-01-02-01-21.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 295 audio samples Skipping file 03-01-02-02-01-01-21.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 299 audio samples Skipping file 03-01-02-01-01-01-19.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 300 audio samples Skipping file 03-01-02-02-02-01-19.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 323 audio samples Skipping file 03-01-02-01-02-02-19.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 325 audio samples Skipping file 03-01-02-02-01-02-19.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 340 audio samples Skipping file 03-01-02-02-02-02-19.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 341 audio samples Skipping file 03-01-02-01-01-02-19.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 345 audio samples Skipping file 03-01-02-02-01-01-19.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 347 audio samples Skipping file 03-01-02-01-02-01-19.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 360 audio samples Skipping file 03-01-02-02-01-01-10.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 361 audio samples Skipping file 03-01-02-01-02-01-10.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 385 audio samples Skipping file 03-01-02-02-02-02-10.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 387 audio samples Skipping file 03-01-02-01-01-02-10.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 399 audio samples Skipping file 03-01-02-01-02-02-10.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 400 audio samples Skipping file 03-01-02-02-01-02-10.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 403 audio samples Skipping file 03-01-02-01-01-01-10.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 405 audio samples Skipping file 03-01-02-02-02-01-10.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 428 audio samples Skipping file 03-01-02-01-02-01-17.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 430 audio samples Skipping file 03-01-02-02-01-01-17.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 434 audio samples Skipping file 03-01-02-01-01-02-17.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 435 audio samples Skipping file 03-01-02-02-02-02-17.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 450 audio samples Skipping file 03-01-02-02-01-02-17.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 452 audio samples Skipping file 03-01-02-01-02-02-17.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 475 audio samples Skipping file 03-01-02-02-02-01-17.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 476 audio samples Skipping file 03-01-02-01-01-01-17.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 480 audio samples Skipping file 03-01-02-02-01-01-04.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 482 audio samples Skipping file 03-01-02-01-02-01-04.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 505 audio samples Skipping file 03-01-02-02-02-02-04.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 506 audio samples Skipping file 03-01-02-01-01-02-04.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 518 audio samples Skipping file 03-01-02-01-02-02-04.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 520 audio samples Skipping file 03-01-02-02-01-02-04.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 524 audio samples Skipping file 03-01-02-01-01-01-04.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 525 audio samples Skipping file 03-01-02-02-02-01-04.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 549 audio samples Skipping file 03-01-02-01-02-01-03.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 550 audio samples Skipping file 03-01-02-02-01-01-03.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 553 audio samples Skipping file 03-01-02-01-01-02-03.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 555 audio samples Skipping file 03-01-02-02-02-02-03.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 570 audio samples Skipping file 03-01-02-02-01-02-03.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 571 audio samples Skipping file 03-01-02-01-02-02-03.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 595 audio samples Skipping file 03-01-02-02-02-01-03.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 597 audio samples Skipping file 03-01-02-01-01-01-03.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 610 audio samples Skipping file 03-01-02-02-01-01-02.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 611 audio samples Skipping file 03-01-02-01-02-01-02.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 615 audio samples Skipping file 03-01-02-02-02-02-02.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 617 audio samples Skipping file 03-01-02-01-01-02-02.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 629 audio samples Skipping file 03-01-02-01-02-02-02.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 630 audio samples Skipping file 03-01-02-02-01-02-02.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 653 audio samples Skipping file 03-01-02-01-01-01-02.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 655 audio samples Skipping file 03-01-02-02-02-01-02.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 658 audio samples Skipping file 03-01-02-01-02-01-05.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 660 audio samples Skipping file 03-01-02-02-01-01-05.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 684 audio samples Skipping file 03-01-02-01-01-02-05.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 685 audio samples Skipping file 03-01-02-02-02-02-05.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 700 audio samples Skipping file 03-01-02-02-01-02-05.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 704 audio samples Skipping file 03-01-02-02-02-01-05.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 705 audio samples Skipping file 03-01-02-01-01-01-05.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 717 audio samples Skipping file 03-01-02-01-02-01-12.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 719 audio samples Skipping file 03-01-02-02-01-01-12.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 743 audio samples Skipping file 03-01-02-01-01-02-12.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 744 audio samples Skipping file 03-01-02-02-02-02-12.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 759 audio samples Skipping file 03-01-02-02-01-02-12.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 761 audio samples Skipping file 03-01-02-01-02-02-12.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 764 audio samples Skipping file 03-01-02-02-02-01-12.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 765 audio samples Skipping file 03-01-02-01-01-01-12.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 789 audio samples Skipping file 03-01-02-02-01-01-15.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 790 audio samples Skipping file 03-01-02-01-02-01-15.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 794 audio samples Skipping file 03-01-02-02-02-02-15.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 796 audio samples Skipping file 03-01-02-01-01-02-15.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 808 audio samples Skipping file 03-01-02-01-02-02-15.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 809 audio samples Skipping file 03-01-02-02-01-02-15.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 832 audio samples Skipping file 03-01-02-01-01-01-15.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 834 audio samples Skipping file 03-01-02-02-02-01-15.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 847 audio samples Skipping file 03-01-02-01-01-01-23.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 849 audio samples Skipping file 03-01-02-02-02-01-23.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 853 audio samples Skipping file 03-01-02-01-02-02-23.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 854 audio samples Skipping file 03-01-02-02-01-02-23.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 869 audio samples Skipping file 03-01-02-02-02-02-23.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 871 audio samples Skipping file 03-01-02-01-01-02-23.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 894 audio samples Skipping file 03-01-02-02-01-01-23.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 895 audio samples Skipping file 03-01-02-01-02-01-23.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 899 audio samples Skipping file 03-01-02-02-02-01-24.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 900 audio samples Skipping file 03-01-02-01-01-01-24.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 924 audio samples Skipping file 03-01-02-02-01-02-24.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 926 audio samples Skipping file 03-01-02-01-02-02-24.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 938 audio samples Skipping file 03-01-02-01-01-02-24.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 939 audio samples Skipping file 03-01-02-02-02-02-24.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 942 audio samples Skipping file 03-01-02-01-02-01-24.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 944 audio samples Skipping file 03-01-02-02-01-01-24.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 969 audio samples Skipping file 03-01-02-02-02-01-22.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 971 audio samples Skipping file 03-01-02-01-01-01-22.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 974 audio samples Skipping file 03-01-02-02-01-02-22.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 975 audio samples Skipping file 03-01-02-01-02-02-22.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 987 audio samples Skipping file 03-01-02-01-01-02-22.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 989 audio samples Skipping file 03-01-02-02-02-02-22.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1013 audio samples Skipping file 03-01-02-01-02-01-22.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1014 audio samples Skipping file 03-01-02-02-01-01-22.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1028 audio samples Skipping file 03-01-02-01-02-01-14.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1029 audio samples Skipping file 03-01-02-02-01-01-14.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1032 audio samples Skipping file 03-01-02-01-01-02-14.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1034 audio samples Skipping file 03-01-02-02-02-02-14.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1049 audio samples Skipping file 03-01-02-02-01-02-14.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1050 audio samples Skipping file 03-01-02-01-02-02-14.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1074 audio samples Skipping file 03-01-02-02-02-01-14.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1076 audio samples Skipping file 03-01-02-01-01-01-14.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1079 audio samples Skipping file 03-01-02-02-01-01-13.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1081 audio samples Skipping file 03-01-02-01-02-01-13.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1104 audio samples Skipping file 03-01-02-02-02-02-13.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1105 audio samples Skipping file 03-01-02-01-01-02-13.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1117 audio samples Skipping file 03-01-02-01-02-02-13.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1119 audio samples Skipping file 03-01-02-02-01-02-13.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1123 audio samples Skipping file 03-01-02-01-01-01-13.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1124 audio samples Skipping file 03-01-02-02-02-01-13.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1149 audio samples Skipping file 03-01-02-02-02-01-09.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1151 audio samples Skipping file 03-01-02-01-01-01-09.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1154 audio samples Skipping file 03-01-02-02-01-02-09.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1155 audio samples Skipping file 03-01-02-01-02-02-09.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1167 audio samples Skipping file 03-01-02-01-01-02-09.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1169 audio samples Skipping file 03-01-02-02-02-02-09.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1193 audio samples Skipping file 03-01-02-01-02-01-09.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1194 audio samples Skipping file 03-01-02-02-01-01-09.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1199 audio samples Skipping file 03-01-02-02-01-01-07.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1200 audio samples Skipping file 03-01-02-01-02-01-07.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1224 audio samples Skipping file 03-01-02-02-02-02-07.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1226 audio samples Skipping file 03-01-02-01-01-02-07.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1238 audio samples Skipping file 03-01-02-01-02-02-07.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1239 audio samples Skipping file 03-01-02-02-01-02-07.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1242 audio samples Skipping file 03-01-02-01-01-01-07.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1244 audio samples Skipping file 03-01-02-02-02-01-07.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1258 audio samples Skipping file 03-01-02-01-02-01-06.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1259 audio samples Skipping file 03-01-02-02-01-01-06.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1282 audio samples Skipping file 03-01-02-01-01-02-06.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1284 audio samples Skipping file 03-01-02-02-02-02-06.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1299 audio samples Skipping file 03-01-02-02-01-02-06.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1300 audio samples Skipping file 03-01-02-01-02-02-06.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1304 audio samples Skipping file 03-01-02-02-02-01-06.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1306 audio samples Skipping file 03-01-02-01-01-01-06.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1329 audio samples Skipping file 03-01-02-02-01-01-01.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1331 audio samples Skipping file 03-01-02-01-02-01-01.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1334 audio samples Skipping file 03-01-02-02-02-02-01.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1345 audio samples Skipping file 03-01-02-01-02-02-01.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1347 audio samples Skipping file 03-01-02-02-01-02-01.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1371 audio samples Skipping file 03-01-02-01-01-01-01.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1372 audio samples Skipping file 03-01-02-02-02-01-01.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1385 audio samples Skipping file 03-01-02-01-01-01-08.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1387 audio samples Skipping file 03-01-02-02-02-01-08.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1391 audio samples Skipping file 03-01-02-01-02-02-08.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1392 audio samples Skipping file 03-01-02-02-01-02-08.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1407 audio samples Skipping file 03-01-02-02-02-02-08.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1409 audio samples Skipping file 03-01-02-01-01-02-08.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1432 audio samples Skipping file 03-01-02-02-01-01-08.wav due to unwanted emotion code.\n",
            "Processed RAVDESS: 1433 audio samples Skipping file 03-01-02-01-02-01-08.wav due to unwanted emotion code.\n",
            "Processed TESS: 4235 audio samples es "
          ]
        }
      ],
      "source": [
        "features, emotions = load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVUa7RAuL9hG",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Let's see what the features we extracted look like, **also for saving both the features matrix as well as emotions array, we need to convert them to pandas dataframe.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "mzxX583yL9hG",
        "outputId": "41ddf378-80f2-4b00-8cdc-6a29836f651d",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Audio samples represented: 4045\n",
            "Numerical features extracted per sample: 180\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.552245</td>\n",
              "      <td>0.504338</td>\n",
              "      <td>0.512778</td>\n",
              "      <td>0.550540</td>\n",
              "      <td>0.572176</td>\n",
              "      <td>0.615567</td>\n",
              "      <td>0.603708</td>\n",
              "      <td>0.573230</td>\n",
              "      <td>0.593454</td>\n",
              "      <td>0.603834</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.543537</td>\n",
              "      <td>-0.912676</td>\n",
              "      <td>-1.747536</td>\n",
              "      <td>-2.299447</td>\n",
              "      <td>-0.025205</td>\n",
              "      <td>2.449470</td>\n",
              "      <td>2.116510</td>\n",
              "      <td>3.361054</td>\n",
              "      <td>2.086241</td>\n",
              "      <td>0.376519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.550732</td>\n",
              "      <td>0.508484</td>\n",
              "      <td>0.466810</td>\n",
              "      <td>0.438485</td>\n",
              "      <td>0.428218</td>\n",
              "      <td>0.445352</td>\n",
              "      <td>0.444528</td>\n",
              "      <td>0.444478</td>\n",
              "      <td>0.500419</td>\n",
              "      <td>0.551672</td>\n",
              "      <td>...</td>\n",
              "      <td>2.333152</td>\n",
              "      <td>3.619088</td>\n",
              "      <td>0.793563</td>\n",
              "      <td>0.743431</td>\n",
              "      <td>0.660723</td>\n",
              "      <td>-0.254914</td>\n",
              "      <td>-0.967536</td>\n",
              "      <td>0.378563</td>\n",
              "      <td>-0.338074</td>\n",
              "      <td>-0.463241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.672768</td>\n",
              "      <td>0.601451</td>\n",
              "      <td>0.548317</td>\n",
              "      <td>0.477806</td>\n",
              "      <td>0.453192</td>\n",
              "      <td>0.516665</td>\n",
              "      <td>0.570100</td>\n",
              "      <td>0.599750</td>\n",
              "      <td>0.647746</td>\n",
              "      <td>0.692147</td>\n",
              "      <td>...</td>\n",
              "      <td>0.283253</td>\n",
              "      <td>0.218117</td>\n",
              "      <td>-1.928165</td>\n",
              "      <td>-2.486516</td>\n",
              "      <td>-0.530147</td>\n",
              "      <td>0.354905</td>\n",
              "      <td>0.463640</td>\n",
              "      <td>0.581006</td>\n",
              "      <td>-2.091462</td>\n",
              "      <td>-1.899596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.602309</td>\n",
              "      <td>0.605448</td>\n",
              "      <td>0.558788</td>\n",
              "      <td>0.529441</td>\n",
              "      <td>0.546190</td>\n",
              "      <td>0.568049</td>\n",
              "      <td>0.566027</td>\n",
              "      <td>0.523464</td>\n",
              "      <td>0.526789</td>\n",
              "      <td>0.552809</td>\n",
              "      <td>...</td>\n",
              "      <td>2.176565</td>\n",
              "      <td>3.904047</td>\n",
              "      <td>0.980935</td>\n",
              "      <td>1.160923</td>\n",
              "      <td>-0.694902</td>\n",
              "      <td>-0.969963</td>\n",
              "      <td>-0.234802</td>\n",
              "      <td>0.941700</td>\n",
              "      <td>0.695973</td>\n",
              "      <td>0.430566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.556555</td>\n",
              "      <td>0.503434</td>\n",
              "      <td>0.473118</td>\n",
              "      <td>0.479222</td>\n",
              "      <td>0.495470</td>\n",
              "      <td>0.522489</td>\n",
              "      <td>0.532196</td>\n",
              "      <td>0.540681</td>\n",
              "      <td>0.545616</td>\n",
              "      <td>0.512521</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.306031</td>\n",
              "      <td>0.050261</td>\n",
              "      <td>-0.992539</td>\n",
              "      <td>-0.213367</td>\n",
              "      <td>-0.351018</td>\n",
              "      <td>-1.462262</td>\n",
              "      <td>-2.454565</td>\n",
              "      <td>-1.044800</td>\n",
              "      <td>-0.298479</td>\n",
              "      <td>0.118208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4040</th>\n",
              "      <td>0.390837</td>\n",
              "      <td>0.321064</td>\n",
              "      <td>0.320337</td>\n",
              "      <td>0.392766</td>\n",
              "      <td>0.461830</td>\n",
              "      <td>0.514541</td>\n",
              "      <td>0.490313</td>\n",
              "      <td>0.429589</td>\n",
              "      <td>0.454518</td>\n",
              "      <td>0.543537</td>\n",
              "      <td>...</td>\n",
              "      <td>4.322751</td>\n",
              "      <td>6.196385</td>\n",
              "      <td>10.026893</td>\n",
              "      <td>9.592944</td>\n",
              "      <td>7.508250</td>\n",
              "      <td>6.768993</td>\n",
              "      <td>1.462945</td>\n",
              "      <td>4.204814</td>\n",
              "      <td>8.904977</td>\n",
              "      <td>5.024060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4041</th>\n",
              "      <td>0.436776</td>\n",
              "      <td>0.408656</td>\n",
              "      <td>0.417071</td>\n",
              "      <td>0.466955</td>\n",
              "      <td>0.607587</td>\n",
              "      <td>0.516233</td>\n",
              "      <td>0.426823</td>\n",
              "      <td>0.386675</td>\n",
              "      <td>0.466787</td>\n",
              "      <td>0.570325</td>\n",
              "      <td>...</td>\n",
              "      <td>5.381444</td>\n",
              "      <td>8.112302</td>\n",
              "      <td>10.220036</td>\n",
              "      <td>5.378716</td>\n",
              "      <td>4.247742</td>\n",
              "      <td>3.527376</td>\n",
              "      <td>-1.950199</td>\n",
              "      <td>2.387534</td>\n",
              "      <td>3.865766</td>\n",
              "      <td>5.672267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4042</th>\n",
              "      <td>0.393377</td>\n",
              "      <td>0.339084</td>\n",
              "      <td>0.307554</td>\n",
              "      <td>0.329559</td>\n",
              "      <td>0.414770</td>\n",
              "      <td>0.538470</td>\n",
              "      <td>0.582712</td>\n",
              "      <td>0.400043</td>\n",
              "      <td>0.361023</td>\n",
              "      <td>0.426620</td>\n",
              "      <td>...</td>\n",
              "      <td>7.888710</td>\n",
              "      <td>5.999340</td>\n",
              "      <td>4.547329</td>\n",
              "      <td>-2.039231</td>\n",
              "      <td>2.943804</td>\n",
              "      <td>2.593581</td>\n",
              "      <td>7.219559</td>\n",
              "      <td>11.672709</td>\n",
              "      <td>12.513518</td>\n",
              "      <td>9.711643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4043</th>\n",
              "      <td>0.463788</td>\n",
              "      <td>0.360805</td>\n",
              "      <td>0.354769</td>\n",
              "      <td>0.406535</td>\n",
              "      <td>0.504329</td>\n",
              "      <td>0.489263</td>\n",
              "      <td>0.481225</td>\n",
              "      <td>0.464283</td>\n",
              "      <td>0.450615</td>\n",
              "      <td>0.449139</td>\n",
              "      <td>...</td>\n",
              "      <td>4.293519</td>\n",
              "      <td>0.572754</td>\n",
              "      <td>8.164567</td>\n",
              "      <td>6.282827</td>\n",
              "      <td>9.507586</td>\n",
              "      <td>9.832229</td>\n",
              "      <td>5.619478</td>\n",
              "      <td>7.357552</td>\n",
              "      <td>12.227635</td>\n",
              "      <td>5.870135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4044</th>\n",
              "      <td>0.271744</td>\n",
              "      <td>0.226202</td>\n",
              "      <td>0.168345</td>\n",
              "      <td>0.244209</td>\n",
              "      <td>0.445436</td>\n",
              "      <td>0.552888</td>\n",
              "      <td>0.391617</td>\n",
              "      <td>0.255266</td>\n",
              "      <td>0.296915</td>\n",
              "      <td>0.357781</td>\n",
              "      <td>...</td>\n",
              "      <td>2.774773</td>\n",
              "      <td>2.517404</td>\n",
              "      <td>11.684411</td>\n",
              "      <td>7.353725</td>\n",
              "      <td>1.393997</td>\n",
              "      <td>4.142126</td>\n",
              "      <td>2.316993</td>\n",
              "      <td>1.992617</td>\n",
              "      <td>10.467948</td>\n",
              "      <td>19.332438</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4045 rows  180 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0         1         2         3         4         5         6    \\\n",
              "0     0.552245  0.504338  0.512778  0.550540  0.572176  0.615567  0.603708   \n",
              "1     0.550732  0.508484  0.466810  0.438485  0.428218  0.445352  0.444528   \n",
              "2     0.672768  0.601451  0.548317  0.477806  0.453192  0.516665  0.570100   \n",
              "3     0.602309  0.605448  0.558788  0.529441  0.546190  0.568049  0.566027   \n",
              "4     0.556555  0.503434  0.473118  0.479222  0.495470  0.522489  0.532196   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "4040  0.390837  0.321064  0.320337  0.392766  0.461830  0.514541  0.490313   \n",
              "4041  0.436776  0.408656  0.417071  0.466955  0.607587  0.516233  0.426823   \n",
              "4042  0.393377  0.339084  0.307554  0.329559  0.414770  0.538470  0.582712   \n",
              "4043  0.463788  0.360805  0.354769  0.406535  0.504329  0.489263  0.481225   \n",
              "4044  0.271744  0.226202  0.168345  0.244209  0.445436  0.552888  0.391617   \n",
              "\n",
              "           7         8         9    ...       170       171        172  \\\n",
              "0     0.573230  0.593454  0.603834  ... -3.543537 -0.912676  -1.747536   \n",
              "1     0.444478  0.500419  0.551672  ...  2.333152  3.619088   0.793563   \n",
              "2     0.599750  0.647746  0.692147  ...  0.283253  0.218117  -1.928165   \n",
              "3     0.523464  0.526789  0.552809  ...  2.176565  3.904047   0.980935   \n",
              "4     0.540681  0.545616  0.512521  ... -1.306031  0.050261  -0.992539   \n",
              "...        ...       ...       ...  ...       ...       ...        ...   \n",
              "4040  0.429589  0.454518  0.543537  ...  4.322751  6.196385  10.026893   \n",
              "4041  0.386675  0.466787  0.570325  ...  5.381444  8.112302  10.220036   \n",
              "4042  0.400043  0.361023  0.426620  ...  7.888710  5.999340   4.547329   \n",
              "4043  0.464283  0.450615  0.449139  ...  4.293519  0.572754   8.164567   \n",
              "4044  0.255266  0.296915  0.357781  ...  2.774773  2.517404  11.684411   \n",
              "\n",
              "           173       174       175       176        177        178        179  \n",
              "0    -2.299447 -0.025205  2.449470  2.116510   3.361054   2.086241   0.376519  \n",
              "1     0.743431  0.660723 -0.254914 -0.967536   0.378563  -0.338074  -0.463241  \n",
              "2    -2.486516 -0.530147  0.354905  0.463640   0.581006  -2.091462  -1.899596  \n",
              "3     1.160923 -0.694902 -0.969963 -0.234802   0.941700   0.695973   0.430566  \n",
              "4    -0.213367 -0.351018 -1.462262 -2.454565  -1.044800  -0.298479   0.118208  \n",
              "...        ...       ...       ...       ...        ...        ...        ...  \n",
              "4040  9.592944  7.508250  6.768993  1.462945   4.204814   8.904977   5.024060  \n",
              "4041  5.378716  4.247742  3.527376 -1.950199   2.387534   3.865766   5.672267  \n",
              "4042 -2.039231  2.943804  2.593581  7.219559  11.672709  12.513518   9.711643  \n",
              "4043  6.282827  9.507586  9.832229  5.619478   7.357552  12.227635   5.870135  \n",
              "4044  7.353725  1.393997  4.142126  2.316993   1.992617  10.467948  19.332438  \n",
              "\n",
              "[4045 rows x 180 columns]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(f\"\\nAudio samples represented: {features.shape[0]}\")\n",
        "print(f\"Numerical features extracted per sample: {features.shape[1]}\")\n",
        "features_df = pd.DataFrame(features)  # make it pretty for display\n",
        "\n",
        "\n",
        "# making dataframe for emotions as well\n",
        "emotions_df = pd.DataFrame(emotions)  # make it pretty for display\n",
        "\n",
        "features_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq083zLEL9hH",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "We have a matrix of dim 1435 x 180. Looks good - 1435 audio samples, one per row, with a series of\n",
        "180 numerical features for each sample.\n",
        "\n",
        "**Each of the 1435 feature arrays has 180 features composed of 12 chromagram pitch classes + 128 mel spectrogram bands + 40 MFC coefficients.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VF2SggHTDqbQ"
      },
      "source": [
        "Now we will save our features matrix and emotions array in excel file we dont have to compute them everytime we run the notebook, we can just load them from the excel file whenever required. Make sure to change the path to according to your drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "u6H8hc6gDtbp"
      },
      "outputs": [],
      "source": [
        "features_df.to_excel(\"MIX_featuresRavdess.xlsx\")\n",
        "emotions_df.to_excel(\"MIX_emotionsRavdess.xlsx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpE5m-5aEyoB"
      },
      "source": [
        "## Load pre-saved Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHPB7dCqEotR"
      },
      "source": [
        "Once saved you only need to load them later by running the cell below, and **skip every cell above** except for the one in which we import libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "-8nlJCESEn56"
      },
      "outputs": [],
      "source": [
        "features_df = pd.read_excel(\"MIX_featuresRavdess.xlsx\", index_col=0)\n",
        "emotions_df = pd.read_excel(\"MIX_emotionsRavdess.xlsx\", index_col=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x2dXjybD1NM"
      },
      "source": [
        "let's see if they have been loaded correctly!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "B5Xo1SVMD0qR",
        "outputId": "98300373-d0fd-4da6-eee2-60668cbc10a9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.552245</td>\n",
              "      <td>0.504338</td>\n",
              "      <td>0.512778</td>\n",
              "      <td>0.550540</td>\n",
              "      <td>0.572176</td>\n",
              "      <td>0.615567</td>\n",
              "      <td>0.603708</td>\n",
              "      <td>0.573230</td>\n",
              "      <td>0.593454</td>\n",
              "      <td>0.603834</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.543537</td>\n",
              "      <td>-0.912676</td>\n",
              "      <td>-1.747536</td>\n",
              "      <td>-2.299447</td>\n",
              "      <td>-0.025205</td>\n",
              "      <td>2.449470</td>\n",
              "      <td>2.116510</td>\n",
              "      <td>3.361054</td>\n",
              "      <td>2.086241</td>\n",
              "      <td>0.376519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.550732</td>\n",
              "      <td>0.508484</td>\n",
              "      <td>0.466810</td>\n",
              "      <td>0.438485</td>\n",
              "      <td>0.428218</td>\n",
              "      <td>0.445352</td>\n",
              "      <td>0.444528</td>\n",
              "      <td>0.444478</td>\n",
              "      <td>0.500419</td>\n",
              "      <td>0.551672</td>\n",
              "      <td>...</td>\n",
              "      <td>2.333152</td>\n",
              "      <td>3.619088</td>\n",
              "      <td>0.793563</td>\n",
              "      <td>0.743431</td>\n",
              "      <td>0.660723</td>\n",
              "      <td>-0.254914</td>\n",
              "      <td>-0.967536</td>\n",
              "      <td>0.378563</td>\n",
              "      <td>-0.338074</td>\n",
              "      <td>-0.463241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.672768</td>\n",
              "      <td>0.601451</td>\n",
              "      <td>0.548317</td>\n",
              "      <td>0.477806</td>\n",
              "      <td>0.453192</td>\n",
              "      <td>0.516665</td>\n",
              "      <td>0.570100</td>\n",
              "      <td>0.599750</td>\n",
              "      <td>0.647746</td>\n",
              "      <td>0.692147</td>\n",
              "      <td>...</td>\n",
              "      <td>0.283253</td>\n",
              "      <td>0.218117</td>\n",
              "      <td>-1.928165</td>\n",
              "      <td>-2.486516</td>\n",
              "      <td>-0.530147</td>\n",
              "      <td>0.354905</td>\n",
              "      <td>0.463640</td>\n",
              "      <td>0.581006</td>\n",
              "      <td>-2.091462</td>\n",
              "      <td>-1.899596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.602309</td>\n",
              "      <td>0.605448</td>\n",
              "      <td>0.558788</td>\n",
              "      <td>0.529441</td>\n",
              "      <td>0.546190</td>\n",
              "      <td>0.568049</td>\n",
              "      <td>0.566027</td>\n",
              "      <td>0.523464</td>\n",
              "      <td>0.526789</td>\n",
              "      <td>0.552809</td>\n",
              "      <td>...</td>\n",
              "      <td>2.176565</td>\n",
              "      <td>3.904047</td>\n",
              "      <td>0.980935</td>\n",
              "      <td>1.160923</td>\n",
              "      <td>-0.694902</td>\n",
              "      <td>-0.969963</td>\n",
              "      <td>-0.234802</td>\n",
              "      <td>0.941700</td>\n",
              "      <td>0.695973</td>\n",
              "      <td>0.430566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.556555</td>\n",
              "      <td>0.503434</td>\n",
              "      <td>0.473118</td>\n",
              "      <td>0.479222</td>\n",
              "      <td>0.495470</td>\n",
              "      <td>0.522489</td>\n",
              "      <td>0.532196</td>\n",
              "      <td>0.540681</td>\n",
              "      <td>0.545616</td>\n",
              "      <td>0.512521</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.306031</td>\n",
              "      <td>0.050261</td>\n",
              "      <td>-0.992539</td>\n",
              "      <td>-0.213367</td>\n",
              "      <td>-0.351018</td>\n",
              "      <td>-1.462262</td>\n",
              "      <td>-2.454565</td>\n",
              "      <td>-1.044800</td>\n",
              "      <td>-0.298479</td>\n",
              "      <td>0.118208</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  180 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2         3         4         5         6    \\\n",
              "0  0.552245  0.504338  0.512778  0.550540  0.572176  0.615567  0.603708   \n",
              "1  0.550732  0.508484  0.466810  0.438485  0.428218  0.445352  0.444528   \n",
              "2  0.672768  0.601451  0.548317  0.477806  0.453192  0.516665  0.570100   \n",
              "3  0.602309  0.605448  0.558788  0.529441  0.546190  0.568049  0.566027   \n",
              "4  0.556555  0.503434  0.473118  0.479222  0.495470  0.522489  0.532196   \n",
              "\n",
              "        7         8         9    ...       170       171       172       173  \\\n",
              "0  0.573230  0.593454  0.603834  ... -3.543537 -0.912676 -1.747536 -2.299447   \n",
              "1  0.444478  0.500419  0.551672  ...  2.333152  3.619088  0.793563  0.743431   \n",
              "2  0.599750  0.647746  0.692147  ...  0.283253  0.218117 -1.928165 -2.486516   \n",
              "3  0.523464  0.526789  0.552809  ...  2.176565  3.904047  0.980935  1.160923   \n",
              "4  0.540681  0.545616  0.512521  ... -1.306031  0.050261 -0.992539 -0.213367   \n",
              "\n",
              "        174       175       176       177       178       179  \n",
              "0 -0.025205  2.449470  2.116510  3.361054  2.086241  0.376519  \n",
              "1  0.660723 -0.254914 -0.967536  0.378563 -0.338074 -0.463241  \n",
              "2 -0.530147  0.354905  0.463640  0.581006 -2.091462 -1.899596  \n",
              "3 -0.694902 -0.969963 -0.234802  0.941700  0.695973  0.430566  \n",
              "4 -0.351018 -1.462262 -2.454565 -1.044800 -0.298479  0.118208  \n",
              "\n",
              "[5 rows x 180 columns]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esInVDq7L9hT"
      },
      "source": [
        "Let's see the class balance of our dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "Si3OFQe7L9hU",
        "outputId": "6d693311-c9fc-42e6-a904-3f05dee87b75"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 3500x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plot emotions\n",
        "plt.figure(figsize=(35, 4))\n",
        "plt.subplot(1, 3, 1)\n",
        "# np.unique returns ordered list of unique elements and count of each element\n",
        "emotion_list, count = np.unique(emotions, return_counts=True)\n",
        "plt.bar(x=range(7), height=count)\n",
        "plt.xticks(ticks=range(7), labels=[emotion for emotion in emotion_list], fontsize=10)\n",
        "plt.xlabel(\"Emotion\")\n",
        "plt.tick_params(labelsize=16)\n",
        "plt.ylabel(\"Number of Samples\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw_wKC8WL9hV"
      },
      "source": [
        "**Great, the classes appear to be balanced. That makes the task easier.** All emotions _except_ the neutral class have a \"strong\" intensity so there are half as many neutral samples. That might have an impact.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhtlYshOL9hV"
      },
      "source": [
        "### Feature Scaling\n",
        "\n",
        "To properly train most machine learning models on _most_ datasets, we first need to scale our features. **This is crucial for models which compute distances between data, and especially critical for DNNs**: If there is a difference in the variance of features simply because of their possible range of values, then a model will learn that the features with the greatest variance are the most important. However, **differences in the variance of unscaled features belonging to different and unknown distributions is an inappropriate measure of importance.** Let's check our features' properties:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IUm1DLwL9hW",
        "outputId": "e4579adb-159a-4a4f-a1f4-2d49dd528fae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12 Chromagram features:           min = 0.154,     max = 0.936,     mean = 0.530,     deviation = 0.134\n",
            "\n",
            "128 Mel Spectrogram features:     min = 0.000,     max = 149.208,     mean = 0.377,     deviation = 2.063\n",
            "\n",
            "40 MFCC features:                 min = -863.639,    max = 120.333,    mean = -9.759,    deviation = 76.973\n"
          ]
        }
      ],
      "source": [
        "# We would usually use df.describe(), but it provides a bit of a mess of information we don't need at the moment.\n",
        "def print_features(df):\n",
        "    # Check chromagram feature values\n",
        "    features_df_chromagram = df.loc[:, :11]\n",
        "    chroma_min = features_df_chromagram.min().min()\n",
        "    chroma_max = features_df_chromagram.max().max()\n",
        "    # stack all features into a single series so we don't get a mean of means or stdev of stdevs\n",
        "    chroma_mean = features_df_chromagram.stack().mean()\n",
        "    chroma_stdev = features_df_chromagram.stack().std()\n",
        "    print(\n",
        "        f\"12 Chromagram features:       \\\n",
        "    min = {chroma_min:.3f}, \\\n",
        "    max = {chroma_max:.3f}, \\\n",
        "    mean = {chroma_mean:.3f}, \\\n",
        "    deviation = {chroma_stdev:.3f}\"\n",
        "    )\n",
        "\n",
        "    # Check mel spectrogram feature values\n",
        "    features_df_melspectrogram = df.loc[:, 12:139]\n",
        "    mel_min = features_df_melspectrogram.min().min()\n",
        "    mel_max = features_df_melspectrogram.max().max()\n",
        "    # stack all features into a single series so we don't get a mean of means or stdev of stdevs\n",
        "    mel_mean = features_df_melspectrogram.stack().mean()\n",
        "    mel_stdev = features_df_melspectrogram.stack().std()\n",
        "    print(\n",
        "        f\"\\n128 Mel Spectrogram features: \\\n",
        "    min = {mel_min:.3f}, \\\n",
        "    max = {mel_max:.3f}, \\\n",
        "    mean = {mel_mean:.3f}, \\\n",
        "    deviation = {mel_stdev:.3f}\"\n",
        "    )\n",
        "\n",
        "    # Check MFCC feature values\n",
        "    features_df_mfcc = df.loc[:, 140:179]\n",
        "    mfcc_min = features_df_mfcc.min().min()\n",
        "    mfcc_max = features_df_mfcc.max().max()\n",
        "    # stack all features into a single series so we don't get a mean of means or stdev of stdevs\n",
        "    mfcc_mean = features_df_mfcc.stack().mean()\n",
        "    mfcc_stdev = features_df_mfcc.stack().std()\n",
        "    print(\n",
        "        f\"\\n40 MFCC features:             \\\n",
        "    min = {mfcc_min:.3f},\\\n",
        "    max = {mfcc_max:.3f},\\\n",
        "    mean = {mfcc_mean:.3f},\\\n",
        "    deviation = {mfcc_stdev:.3f}\"\n",
        "    )\n",
        "\n",
        "\n",
        "print_features(features_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFgQlYSlL9hW"
      },
      "source": [
        "**There's an obvious imbalance in the variance our features; Our features indeed belong to very different distributions:** our MFC coefficients' deviation is greater than the other features by orders of magnitude. That does not mean MFC coefficients are the most important feature, but rather it is a property of the way they are computed. We will certainly need to scale this feature set.\n",
        "\n",
        "We have the choice of sklearn's StandardScaler and MinMaxScaler. Standard scaling subtracts the mean of each feature and divides it by the standard deviation of that feature, producing features with mean at zero and unit variance - that is, a variance and standard deviation of 1. Min-Max scaling transforms each feature to be within a bounded interval that we specify.\n",
        "\n",
        "In practice, **MinMax scaling is especially useful when we know our features should be in a bounded interval**, such as pixel values in [0,255], while **standard scaling is perhaps more practical for features with unknown distributions** because centering the features at zero-mean with a standard deviation of 1 means extreme values will have less of an impact on the model's learned weights, i.e. the model is less sensitive to outliers.\n",
        "\n",
        "We'll create MinMax scaled features as well so we can give them a try later on to confirm that standard scaling is better in the absence of knowledge on the appropriate distribution for a dataset's features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "_BCAYVEUL9hW"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "# keep our unscaled features just in case we need to process them alternatively\n",
        "features_scaled = features\n",
        "features_scaled = scaler.fit_transform(features_scaled)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "# keep our unscaled features just in case we need to process them alternatively\n",
        "features_minmax = features\n",
        "features_minmax = scaler.fit_transform(features_minmax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4ATx5oNL9hX"
      },
      "source": [
        "Make sure our features are properly scaled:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlRuHQkKL9hX",
        "outputId": "4bd31c3c-47ed-4f16-d45c-0767ceb48da8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mStandard Scaling:\n",
            "\u001b[0m\n",
            "12 Chromagram features:           min = -2.951,     max = 2.931,     mean = -0.000,     deviation = 1.000\n",
            "\n",
            "128 Mel Spectrogram features:     min = -0.538,     max = 55.963,     mean = -0.000,     deviation = 1.000\n",
            "\n",
            "40 MFCC features:                 min = -4.196,    max = 7.361,    mean = 0.000,    deviation = 1.000\n",
            "\n",
            "\n",
            "\u001b[1mMinMax Scaling:\n",
            "\u001b[0m\n",
            "12 Chromagram features:           min = 0.000,     max = 1.000,     mean = 0.506,     deviation = 0.195\n",
            "\n",
            "128 Mel Spectrogram features:     min = 0.000,     max = 1.000,     mean = 0.015,     deviation = 0.052\n",
            "\n",
            "40 MFCC features:                 min = 0.000,    max = 1.000,    mean = 0.397,    deviation = 0.189\n"
          ]
        }
      ],
      "source": [
        "print(\"\\033[1m\" + \"Standard Scaling:\\n\" + \"\\033[0m\")\n",
        "features_scaled_df = pd.DataFrame(features_scaled)\n",
        "print_features(features_scaled_df)\n",
        "\n",
        "print(\"\\n\\n\\033[1m\" + \"MinMax Scaling:\\n\" + \"\\033[0m\")\n",
        "features_minmax_df = pd.DataFrame(features_minmax)\n",
        "print_features(features_minmax_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZfzIh7DL9hX"
      },
      "source": [
        "Perfect. Zero mean and unit variance for standard scaling and in the range [0,1] for MinMax scaling - a default when we don't specify values. We can now move on to building predictive models for these features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5tXewLEL9hX"
      },
      "source": [
        "## Classical Machine Learning Models\n",
        "\n",
        "Classical machine learning models encompass a broad range of algorithms that have been foundational to the field's development and are still widely used for various predictive tasks. These models can be broadly categorized into supervised and unsupervised learning methods, each suited for different kinds of data and objectives.\n",
        "\n",
        "We will be looking into few popular Machine Learning Algorithms such as Support Vector Machine(SVM), K-Nearest Neighbors and Random Forest Classifier. There are many other classical models with their own strengths and weaknesses, and the choice of model depends on the specific requirements of the task, including the nature of the data, the complexity of the problem, and the computational efficiency required. Despite the rise of deep learning, classical machine learning models remain vital tools in a data scientist's arsenal due to their efficiency, interpretability, and strong performance in many scenarios.\n",
        "\n",
        "The use of classic machine learning method is due to the small size of our dataset; Some of the most robust models such as Support vector (machine) classifiers **(SVC) and k-Nearest-Neighbour classifiers (kNN) are particularly suited to smaller datasets and fall apart with huge datasets.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwU-jXSGL9hY"
      },
      "source": [
        "### Training: The 80/20 Split and Validation\n",
        "\n",
        "In order to compare models, we'll have to evaluate their performance. The simplest method to do so is to train a model on a portion of our dataset and test it on the remainder. We'll use sklearn's train_test_split to create a standard 80/20 train/test split. The model is fit on 80% of\n",
        "the data and tested for performance against 20% of the data, which it has never seen in training - also called the hold-out set.\n",
        "<img src=\"https://github.com/IAT-ExploringAI-2024/Week3-Machine_Learning/blob/main/images/Capture2.PNG?raw=true\" width=\"800\">\n",
        "\n",
        "More accurately, the proper modality for training and scoring a model is to\n",
        "\n",
        "1. Fit/train our model on a _training_ set,\n",
        "2. Evaluate the model on a _validation_ set to tune the hyperparameters for better performance,\n",
        "3. Finally score our model's true performance - its **generalizability** - against a _test_ set, aka the hold-out set.\n",
        "4. Repeat from 2. **Do not tune the model to score well on the test set**.\n",
        "\n",
        "Different set ratios are used in this approach - a usual example is 60/20/20 train/validation/test.For simplicity, we're going to start with an 80/20 train/test split. The model will be trained on all the training data, and we will check its performance on the test data.\n",
        "\n",
        " <img src=\"https://github.com/IAT-ExploringAI-2024/Week3-Machine_Learning/blob/main/images/traintestsplit.PNG?raw=true\" width=\"800\">\n",
        "\n",
        "Define unscaled and scaled training and test sets:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "8jGN4ROVL9hY"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "############# Unscaled test/train set #############\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    features, emotions, test_size=0.2, random_state=69\n",
        ")\n",
        "\n",
        "############ Standard Scaled test/train set ###########\n",
        "# The labels/classes (y_train, y_test) never change, keep old values\n",
        "X_train_scaled, X_test_scaled, _, _ = train_test_split(\n",
        "    features_scaled, emotions, test_size=0.2, random_state=69\n",
        ")\n",
        "\n",
        "############# MinMax Scaled test/train set ###############\n",
        "# The labels/classes (y_train, y_test) never change, keep old values\n",
        "X_train_minmax, X_test_minmax, _, _ = train_test_split(\n",
        "    features_minmax, emotions, test_size=0.2, random_state=69\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3Mh0RrEL9hY"
      },
      "source": [
        "### Comparing Models\n",
        "\n",
        "We'll try each off-the-shelf machine learning model from sklearn and pick a few to explore, since these models will train near instantly on this dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "iLPeyR7vL9he",
        "outputId": "067b25d8-9f5e-4d9c-f35f-cefcda65090a",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Classifier</th>\n",
              "      <th>Accuracy Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KNeighborsClassifier</td>\n",
              "      <td>85.54%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>84.05%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SVC</td>\n",
              "      <td>79.23%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SVC RBF kernel</td>\n",
              "      <td>78.86%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DecisionTreeClassifier</td>\n",
              "      <td>72.81%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>QuadraticDiscriminantAnalysis</td>\n",
              "      <td>67.49%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GaussianNB</td>\n",
              "      <td>56.98%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>AdaBoostClassifier</td>\n",
              "      <td>38.69%</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Classifier Accuracy Score\n",
              "0           KNeighborsClassifier         85.54%\n",
              "4         RandomForestClassifier         84.05%\n",
              "1                            SVC         79.23%\n",
              "2                 SVC RBF kernel         78.86%\n",
              "3         DecisionTreeClassifier         72.81%\n",
              "7  QuadraticDiscriminantAnalysis         67.49%\n",
              "6                     GaussianNB         56.98%\n",
              "5             AdaBoostClassifier         38.69%"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "\n",
        "classification_models = [\n",
        "    KNeighborsClassifier(),  # (3),\n",
        "    SVC(kernel=\"linear\"),  # , C=0.025),\n",
        "    SVC(kernel=\"rbf\"),\n",
        "    DecisionTreeClassifier(),  # max_depth=5),\n",
        "    RandomForestClassifier(),  # max_depth=5, n_estimators=10, max_features=1),\n",
        "    AdaBoostClassifier(),\n",
        "    GaussianNB(),\n",
        "    QuadraticDiscriminantAnalysis(),\n",
        "]\n",
        "\n",
        "scores = []\n",
        "for model in classification_models:\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    score = model.score(X_test_scaled, y_test)\n",
        "    model_name = type(model).__name__\n",
        "    if model_name == \"SVC\" and model.kernel == \"rbf\":\n",
        "        model_name += \" RBF kernel\"\n",
        "    scores.append((model_name, (f\"{100*score:.2f}%\")))\n",
        "# Make it pretty\n",
        "scores_df = pd.DataFrame(scores, columns=[\"Classifier\", \"Accuracy Score\"])\n",
        "scores_df.sort_values(by=\"Accuracy Score\", axis=0, ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrD98CpPL9he"
      },
      "source": [
        "Let's pick the top three - Random Forests, SVC, and kNN - and take a closer look at each of them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dBNoEolL9hf"
      },
      "source": [
        "### The Support Vector Machine Classifier\n",
        "\n",
        "We'll go in chronological order. First is the support vector machine classifier (SVC) - a model from the 60s. SVMs are models quick to train for this task and best suited to small datasets due to its quadratic time complexity w.r.t. size of the training dataset (# of training samples). This is also the reason it breaks down with larger datasets since it becomes very expensive to train.\n",
        "\n",
        "The idea behind SVMs on which the SVC model is based is to find a separating hyperplane - a subspace with dimension one less than that of the feature space - for points in our feature space; i.e. for a 3D space, a hyperplane is a regular plane, in 2D, a line. This idea extends to n dimensions. If points are separable by a hyperplane, they are said to be linearly separable. **Since there are infinite possible separating hyperplanes for any linearly separable feature space, an SVM computes which points are closest to each such hyperplane and uses them to construct a _support vector_. The SVM picks the hyperplane which maximizes the distance - _margin_ - to each support vector.** In this way, we maximize the separating ability of the chosen hyperplane.\n",
        "\n",
        "The core of SVMs is the kernel. We could map all new points from our input space, where they were not separable by a hyperplane, to a higher dimension in which we have found a hyperplane to separate the points in that space. However, that would be extremely computationally expensive for data that needs to be mapped to much higher dimensions. Instead, we **compute the hyperplane in the higher dimension on our training data and map the hyperplane back to the lower-dimension input space to use for classifying our data. This is the _kernel trick_, whereby the kernel (function) enables us to compute distances to new points in the input space without transforming each to the higher dimensional space - drastically reducing the computational complexity of the SVM.**\n",
        "\n",
        " <img src=\"https://github.com/IAT-ExploringAI-2024/Week3-Machine_Learning/blob/main/images/kernel1.png?raw=true\" width=\"800\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-VTE6HjL9hf"
      },
      "source": [
        "A linear kernel should always be tested because **a linear kernel is much faster to train than a non-linear kernel**; however, properly tuned, a non-linear kernel often provides the best possible predictive performance. **RBF (radial basis function) is a good default to use for a non-linear kernel** and often is the best non-linear kernel because it usually provides a higher accuracy compared to other non-linear kernels at the cost of higher computational complexity. We can afford to try the RBF kernel because our dataset is small.\n",
        "\n",
        "If you want to explore further please have a look at [this article](https://www.analyticsvidhya.com/blog/2021/10/support-vector-machinessvm-a-complete-guide-for-beginners/)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruBbt50ML9hf",
        "outputId": "9f92107e-3784-4ff9-d143-651068f65b7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVC Model's accuracy on training set is 92.43%\n",
            "SVC Model's accuracy on test set is 78.37%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "model = SVC(\n",
        "    C=10,  # higher the value tighter the margin\n",
        "    kernel=\"linear\",\n",
        "    random_state=69,\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\n",
        "    f\"SVC Model's accuracy on training set is {100*model.score(X_train, y_train):.2f}%\"\n",
        ")\n",
        "print(f\"SVC Model's accuracy on test set is {100*model.score(X_test, y_test):.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIm1a-tiL9hg"
      },
      "source": [
        "Not bad at all for the relatively simple SVC model. **Hyperparameter  regulates the margin.** It might do well to optimize the SVC model further if we don't find a better one. As it stands, we are looking for considerably higher performance in this task.\n",
        "\n",
        "Check out [this link](https://towardsdatascience.com/visualizing-the-effect-of-hyperparameters-on-support-vector-machines-b9eef6f7357b) for visual representation of affect of changes in C and gamma.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j72LJRfCL9hg"
      },
      "source": [
        "### k Nearest Neighbours\n",
        "\n",
        "k Nearest Neighbours (kNN) is next in line, a tried-and-true machine learning method from the 70s. kNN makes a lot of intuitive sense: imagine plotting points on a graph and drawing gates around points that look like they belong to the same group. That's what it is - we **plot our training samples' features and compare a test sample's features' distance to all those points; then just take the _k_ closest points to the test sample and pick the most frequent label/class.** That's it.\n",
        "\n",
        "kNN is a great starting point for multiclass problems with small datasets, although on large dadtasets less reliable and extremely memory hungry (it stores all training sample points). kNN is also useful in that it makes **no assumptions about the underlying distribution of the data set - so kNNs work well for both linear and non-linear data.** In the 2D example:\n",
        "\n",
        "<img src=\"https://github.com/IAT-ExploringAI-2024/Week3-Machine_Learning/blob/main/images/knn.png?raw=true\" width=400 height=400 />\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaeCneMiL9hg",
        "outputId": "5c771b63-ba5d-4ef2-93be-eee445e3332d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Default kNN Model's accuracy on training set is 88.47%\n",
            "Default kNN Model's accuracy on test set is 80.72%\n",
            "\n",
            "kNN Model's accuracy on training set is 100.00%\n",
            "kNN Model's accuracy on test set is 83.19%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "####### Default kNN  ########\n",
        "model = KNeighborsClassifier()\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\n",
        "    f\"Default kNN Model's accuracy on training set is {100*model.score(X_train, y_train):.2f}%\"\n",
        ")\n",
        "print(\n",
        "    f\"Default kNN Model's accuracy on test set is {100*model.score(X_test, y_test):.2f}%\\n\"\n",
        ")\n",
        "\n",
        "##### (hastily) tuned kNN ######\n",
        "model = KNeighborsClassifier(\n",
        "    n_neighbors=5, weights=\"distance\", algorithm=\"brute\", n_jobs=4\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\n",
        "    f\"kNN Model's accuracy on training set is {100*model.score(X_train, y_train):.2f}%\"\n",
        ")\n",
        "print(f\"kNN Model's accuracy on test set is {100*model.score(X_test, y_test):.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxVEKUyiL9hh"
      },
      "source": [
        "**The brute-force algorithm computes distances between all pairs of points in the training set; works especially well for small datasets** but wildly inefficient w.r.t. increasing samples and feature space dimension. Not bad for 2 minutes of work, but still not suitable for this task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvQ3av4cL9hh"
      },
      "source": [
        "### Random Forests\n",
        "\n",
        "Finally, and before resorting to deep learning methods, let's try a Random Forest - a model from the 21st century (2001). **We train many distinct decision trees which are essentially directed acyclic graphs (DAGs), somewhat similar to a flow chart. The collection of (decision) trees makes up our Random Forest.**\n",
        "\n",
        "At each node of the tree we have a function (a rule) that evaluates whether the features of samples input to that node belong to one class or another. Each branch of the tree (or, edge of the graph) defines one of two possible results from a node, and each leaf is one of two decisions made by its parent node. **Each tree in the forest evaluates a random subset of the training samples' features and has a rule at each level of the tree that classifies based on these random features - hence, _Random_ Forest. This random selection of features makes Random Forests robust to outliers**, as such features will have less of an impact in the scope of the entire forest, most of whose trees operate on the \"real\" features.\n",
        "\n",
        "**Random Forests are excellent models to use as a benchmark due to their low time complexity to train and because it is an ensemble method, their robustness to unknown distributions and outliers in the dataset,** meaning Random Forests require relatively little exploratory analysis in both the data and training the model to get an idea of their performance in a task.\n",
        "\n",
        "<img src=\"https://github.com/IliaZenkov/sklearn-audio-classification/blob/master/img/randomforest.png?raw=true\" width=500 height=500 />\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9BWmcV9L9hh",
        "outputId": "95d3ddff-07d3-4bbf-d2c4-e14833871d77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Default Random Forest Model's accuracy on training set is 100.00%\n",
            "Default Random Forest Model's accuracy on test set is 84.80%\n",
            "\n",
            "Random Forest Model's accuracy on training set is 100.00%\n",
            "Random Forest Model's accuracy on test set is 84.80%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "####### Default Random Forest ########\n",
        "model = RandomForestClassifier(random_state=69)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\n",
        "    f\"Default Random Forest Model's accuracy on training set is {100*model.score(X_train, y_train):.2f}%\"\n",
        ")\n",
        "print(\n",
        "    f\"Default Random Forest Model's accuracy on test set is {100*model.score(X_test, y_test):.2f}%\\n\"\n",
        ")\n",
        "\n",
        "\n",
        "########## Tuned Random Forest #######\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators=500,\n",
        "    criterion=\"entropy\",\n",
        "    warm_start=True,\n",
        "    max_features=\"sqrt\",\n",
        "    oob_score=True,  # more on this below\n",
        "    random_state=69,\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\n",
        "    f\"Random Forest Model's accuracy on training set is {100*model.score(X_train, y_train):.2f}%\"\n",
        ")\n",
        "print(\n",
        "    f\"Random Forest Model's accuracy on test set is {100*model.score(X_test, y_test):.2f}%\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM9-k8KLL9hi"
      },
      "source": [
        "Not bad for zero effort put into the default model. **Random Forests make a good benchmark model**, especially when strapped for time.\n",
        "\n",
        "**_Max features_ defines size of random feature subset decided upon at each node; sqrt(#features) is a good default for classification.**\n",
        "\n",
        "**_Gini_ and _Entropy_ are functions computing quality of classified samples within each node; they almost always provide similar performance but Entropy is more suited to classification while Gini is better for continuous variables.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnhnxuGXL9hi"
      },
      "source": [
        "As wonderful as Random Forests are, it's clear that we're going to need to pull out bigger guns if we want to get appreciable performance on this dataset, perhaps even with good generalizability on test data. DNNs(Deep Neural Networks) are the next step-up in complexity from classical machine learning models, and we will start at the first rung on that ladder:Simple Perceptron in next lab!\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
